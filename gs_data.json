{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "GCOVDKoAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Bin Lin, 林彬", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=GCOVDKoAAAAJ&citpid=1", "affiliation": "Master student, Peking University", "organization": 10725744176602846184, "interests": ["computer vision", "multimodal LLM"], "email_domain": "@stu.pku.edu.cn", "homepage": "https://linb203.github.io/", "citedby": 1827, "publications": {"GCOVDKoAAAAJ:UebtZRa9Y70C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection", "pub_year": "2023", "citation": "EMNLP 2024, 2023"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:UebtZRa9Y70C", "num_citations": 751, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7371115936063640905,6629867110679209960,10786947121568243742,1322699232764920548", "cites_id": ["7371115936063640905", "6629867110679209960", "10786947121568243742", "1322699232764920548"]}, "GCOVDKoAAAAJ:Se3iqnhoufwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models", "pub_year": "2024", "citation": "arXiv preprint arXiv:2401.15947, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:Se3iqnhoufwC", "num_citations": 271, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12775364500475041691,2984471306207853353,6722487317014063910,17502742909557844813,17770944915125910,13904195723770548633", "cites_id": ["12775364500475041691", "2984471306207853353", "6722487317014063910", "17502742909557844813", "17770944915125910", "13904195723770548633"]}, "GCOVDKoAAAAJ:0EnyYjriUFMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment", "pub_year": "2023", "citation": "ICLR 2024, 2023"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:0EnyYjriUFMC", "num_citations": 259, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1118289122120059240,13327988472277946694,9489950097885075408", "cites_id": ["1118289122120059240", "13327988472277946694", "9489950097885075408"]}, "GCOVDKoAAAAJ:MXK_kJrjxJIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Sharegpt4video: Improving video understanding and generation with better captions", "pub_year": "2024", "citation": "NeurIPS 2024, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:MXK_kJrjxJIC", "num_citations": 190, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5437535431763980848", "cites_id": ["5437535431763980848"]}, "GCOVDKoAAAAJ:hqOjcs7Dif8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models", "pub_year": "2023", "citation": "arXiv preprint arXiv:2311.16103, 2023"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:hqOjcs7Dif8C", "num_citations": 77, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6967765230383967929", "cites_id": ["6967765230383967929"]}, "GCOVDKoAAAAJ:YOwf2qJgpHMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Open-Sora Plan: Open-Source Large Video Generation Model", "pub_year": "2024", "citation": "arXiv preprint arXiv:2412.00131, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:YOwf2qJgpHMC", "num_citations": 69, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10651957277949914530", "cites_id": ["10651957277949914530"]}, "GCOVDKoAAAAJ:5nxA0vEk-isC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators", "pub_year": "2024", "citation": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:5nxA0vEk-isC", "num_citations": 39, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2417688504068971649", "cites_id": ["2417688504068971649"]}, "GCOVDKoAAAAJ:mVmsd5A6BfQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mora: Enabling generalist video generation via a multi-agent framework", "pub_year": "2024", "citation": "arXiv preprint arXiv:2403.13248, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:mVmsd5A6BfQC", "num_citations": 29, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17357860981809691426", "cites_id": ["17357860981809691426"]}, "GCOVDKoAAAAJ:3fE2CSJIrl8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Cycle3d: High-quality and consistent image-to-3d generation via generation-reconstruction cycle", "pub_year": "2025", "citation": "Proceedings of the AAAI Conference on Artificial Intelligence 39 (7), 7320-7328, 2025"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:3fE2CSJIrl8C", "num_citations": 22, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16895990857564126296", "cites_id": ["16895990857564126296"]}, "GCOVDKoAAAAJ:4DMP91E08xMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Wise: A world knowledge-informed semantic evaluation for text-to-image generation", "pub_year": "2025", "citation": "arXiv preprint arXiv:2503.07265, 2025"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:4DMP91E08xMC", "num_citations": 20, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=382394676351522726", "cites_id": ["382394676351522726"]}, "GCOVDKoAAAAJ:LkGwnXOMwfcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "BASALT refines binning from metagenomic data and increases resolution of genome-resolved metagenomic analysis", "pub_year": "2024", "citation": "Nature Communications 15 (1), 2179, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:LkGwnXOMwfcC", "num_citations": 20, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17138345057513091697", "cites_id": ["17138345057513091697"]}, "GCOVDKoAAAAJ:KlAtU1dfN6UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model", "pub_year": "2024", "citation": "ICME 2025, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:KlAtU1dfN6UC", "num_citations": 17, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=18267596157755357113", "cites_id": ["18267596157755357113"]}, "GCOVDKoAAAAJ:ULOm3_A8WrAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model", "pub_year": "2024", "citation": "CVPR 2025, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:ULOm3_A8WrAC", "num_citations": 15, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10223135493305164024", "cites_id": ["10223135493305164024"]}, "GCOVDKoAAAAJ:8k81kl-MbHgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark", "pub_year": "2024", "citation": "arXiv preprint arXiv:2404.09619, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:8k81kl-MbHgC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17494454195110449698", "cites_id": ["17494454195110449698"]}, "GCOVDKoAAAAJ:roLk4NBRz8UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "LLMBind: A unified modality-task integration framework", "pub_year": "2024", "citation": "arXiv preprint arXiv:2402.14891, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:roLk4NBRz8UC", "num_citations": 11, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7918389297639542635", "cites_id": ["7918389297639542635"]}, "GCOVDKoAAAAJ:M3ejUd6NZC8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "TaxDiff: taxonomic-guided diffusion model for protein sequence generation", "pub_year": "2024", "citation": "NeurIPS 2024 Workshop on AI for New Drug Modalities, 2024, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:M3ejUd6NZC8C", "num_citations": 10, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2259306290938859193,6836262672236478579", "cites_id": ["2259306290938859193", "6836262672236478579"]}, "GCOVDKoAAAAJ:_kc_bZDykSQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Next patch prediction for autoregressive visual generation", "pub_year": "2024", "citation": "arXiv preprint arXiv:2412.15321, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:_kc_bZDykSQC", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5539329410314752958", "cites_id": ["5539329410314752958"]}, "GCOVDKoAAAAJ:Wp0gIr-vW9MC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DreamDance: Animating Human Images by Enriching 3D Geometry Cues from 2D Poses", "pub_year": "2024", "citation": "arXiv preprint arXiv:2412.00397, 2024"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:Wp0gIr-vW9MC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6668694555430199287", "cites_id": ["6668694555430199287"]}, "GCOVDKoAAAAJ:-f6ydRqryjwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Imgedit: A unified image editing dataset and benchmark", "pub_year": "2025", "citation": "arXiv preprint arXiv:2505.20275, 2025"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:-f6ydRqryjwC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5549241643893897719", "cites_id": ["5549241643893897719"]}, "GCOVDKoAAAAJ:hFOr9nPyWt4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "pub_year": "2025", "citation": "arXiv preprint arXiv:2506.03147, 2025"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:hFOr9nPyWt4C", "num_citations": 0}, "GCOVDKoAAAAJ:aqlVkmm33-oC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SwapAnyone: Consistent and Realistic Video Synthesis for Swapping Any Person into Any Video", "pub_year": "2025", "citation": "arXiv preprint arXiv:2503.09154, 2025"}, "filled": false, "author_pub_id": "GCOVDKoAAAAJ:aqlVkmm33-oC", "num_citations": 0}}, "citedby5y": 1827, "hindex": 14, "hindex5y": 14, "i10index": 16, "i10index5y": 16, "cites_per_year": {"2023": 13, "2024": 818, "2025": 996}, "updated": "2025-06-14 09:33:12.921332"}